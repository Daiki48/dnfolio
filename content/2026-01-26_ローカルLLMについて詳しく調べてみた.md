+++
title = "ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã¤ã„ã¦è©³ã—ãèª¿ã¹ã¦ã¿ãŸ"
slug = "local-llm-guide-2026"
description = "ãƒ­ãƒ¼ã‚«ãƒ«LLMã£ã¦ä½•ï¼Ÿã©ã‚“ãªå ´é¢ã§ä½¿ã†ã®ï¼Ÿå„ç¤¾APIã¨ã®é•ã„ã¯ï¼Ÿä¸­å›½è£½ãƒ¢ãƒ‡ãƒ«ã®æ‡¸å¿µã¯ï¼Ÿ2026å¹´1æœˆæ™‚ç‚¹ã®å…¬å¼æƒ…å ±ã‚’å…ƒã«ã€ä¼æ¥­ã§ã®LLMé¸å®šã«å½¹ç«‹ã¤æƒ…å ±ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚"
[taxonomies]
tags = ["LLM", "AI", "Cloudflare", "AWS", "GCP", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£"]
languages = ["ja"]
+++

## ã¯ã˜ã‚ã«

ç§ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMçµŒé¨“ã¯ã€ŒCloudflare Workers AIã§Gemma 3ã‚’å‹•ã‹ã—ã¦ã¿ãŸã€ç¨‹åº¦ã®æµ…ã„çŸ¥è­˜ã—ã‹ãªã„ã€‚

æ™®æ®µã¯Claude Codeã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ãŸã‚Šã€claude.aiã‚„Geminiã§ç›¸è«‡ã—ãŸã‚Šã€‚Google AI Proãƒ—ãƒ©ãƒ³ã«åŠ å…¥ã—ã¦ã„ã‚‹ã®ã§ã€NotebookLMã¨ã‹Googleé–¢é€£ã®ã‚µãƒ¼ãƒ“ã‚¹ã§Geminiã‚’ã¡ã‚‡ã£ã¨è§¦ã‚‹ãã‚‰ã„ã€‚ãƒ¡ã‚¤ãƒ³ã¯Claude Codeã§ã®é–‹ç™ºãªã®ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã¯æ­£ç›´ã‚ã¾ã‚Šç¸ãŒãªã‹ã£ãŸã€‚

ãã‚“ãªç§ãŒãªã‚“ã§ã“ã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ã‚‹ã‹ã¨ã„ã†ã¨ã€ä»•äº‹ã§LLMã®é¸å®šã‚’è€ƒãˆã‚‹æ©Ÿä¼šãŒã‚ã£ã¦ã€ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã£ã¦å®Ÿéš›ã©ã†ãªã®ï¼Ÿã€ã€Œä¸­å›½è£½ãƒ¢ãƒ‡ãƒ«ä½¿ã£ã¦å¤§ä¸ˆå¤«ãªã®ï¼Ÿã€ã¿ãŸã„ãªç–‘å•ãŒæ¹§ã„ã¦ããŸã‹ã‚‰ã€‚

èª¿ã¹ã¦ã¿ãŸã‚‰æ„å¤–ã¨æƒ…å ±ãŒæ•£ã‚‰ã°ã£ã¦ã„ã¦ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¨ªæ–­çš„ã«ã¾ã¨ã‚ãŸè¨˜äº‹ãŒã‚ã¾ã‚Šãªã‹ã£ãŸã€‚ãªã®ã§ã€è‡ªåˆ†ã®å‹‰å¼·ã‚‚å…¼ã­ã¦æ•´ç†ã—ã¦ã¿ãŸã€‚

åŒã˜ã‚ˆã†ã«è¿·ã£ã¦ã„ã‚‹äººã®å‚è€ƒã«ãªã‚Œã°å¬‰ã—ã„ã€‚

---

## ãã‚‚ãã‚‚ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã€ã£ã¦ä½•ï¼Ÿ

ChatGPTã‚„Claudeã‚’ä½¿ã£ãŸã“ã¨ãŒã‚ã‚‹äººã¯å¤šã„ã¨æ€ã†ã€‚ã‚ã‚Œã¯ã€Œã‚¯ãƒ©ã‚¦ãƒ‰APIå‹ã€ã®LLMã€‚å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¯OpenAIã‚„Anthropicã®ã‚µãƒ¼ãƒãƒ¼ã«é€ã‚‰ã‚Œã¦ã€ãã“ã§å‡¦ç†ã•ã‚Œã¦çµæœãŒè¿”ã£ã¦ãã‚‹ä»•çµ„ã¿ã ã€‚

ä¸€æ–¹ã€**ãƒ­ãƒ¼ã‚«ãƒ«LLM**ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆLLMã€ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹LLMã¨ã‚‚å‘¼ã°ã‚Œã‚‹ï¼‰ã¯ã€**è‡ªåˆ†ã®ç®¡ç†ä¸‹ã«ã‚ã‚‹ã‚µãƒ¼ãƒãƒ¼ã‚„PCã§LLMã‚’å‹•ã‹ã™**æ–¹å¼ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤–éƒ¨ã«å‡ºã‚‹ã“ã¨ãªãã€å®Œå…¨ã«è‡ªåˆ†ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ä¸‹ã§æ¨è«–å‡¦ç†ã‚’è¡Œã†ã€‚

å…·ä½“çš„ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå½¢æ…‹ãŒã‚ã‚‹ï¼š

| å½¢æ…‹ | èª¬æ˜ |
|------|------|
| **è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼** | è‡ªç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã‚„ã‚ªãƒ•ã‚£ã‚¹å†…ã®ã‚µãƒ¼ãƒãƒ¼ã§LLMã‚’ç¨¼åƒ |
| **ã‚¯ãƒ©ã‚¦ãƒ‰GPU** | AWSã€GCPã€Cloudflareãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã®GPUã‚’å€Ÿã‚Šã¦LLMã‚’ç¨¼åƒ |
| **ãƒ­ãƒ¼ã‚«ãƒ«PC** | é–‹ç™ºè€…ã®æ‰‹å…ƒã®PCã§LLMã‚’ç¨¼åƒï¼ˆä¸»ã«é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨é€”ï¼‰ |

ç§ãŒè©¦ã—ãŸCloudflare Workers AIï¼ˆGemma 3ã®12Bï¼‰ã¯ã€Œã‚¯ãƒ©ã‚¦ãƒ‰GPUã€ã«è©²å½“ã™ã‚‹ã€‚CloudflareãŒãƒ›ã‚¹ãƒˆã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã€è‡ªåˆ†ã®Workerã‹ã‚‰å‘¼ã³å‡ºã™å½¢å¼ã ã€‚ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæ¥½ã ã£ãŸã®ã§ã€ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMå…¥é–€ã€ã¨ã—ã¦ã¯æ‚ªããªã‹ã£ãŸã¨æ€ã†ã€‚

---

## ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒå¿…è¦ã«ãªã‚‹å ´é¢

ã€Œã§ã€çµå±€ã„ã¤ä½¿ã†ã®ï¼Ÿã€ã¨ã„ã†è©±ã€‚

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãŒæœ€å„ªå…ˆã®å ´åˆ

é‡‘èã€åŒ»ç™‚ã€æ³•å‹™ã€æ”¿åºœæ©Ÿé–¢ãªã©ã€**æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã‚’å¤–éƒ¨ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã§ããªã„**æ¥­ç¨®ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒç¾å®Ÿçš„ãªé¸æŠè‚¢ã«ãªã‚‹ã€‚

è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã›ã°ã€æ©Ÿå¯†æƒ…å ±ãŒå¤–éƒ¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã•ã‚Œã‚‹ã“ã¨ã‚’é¿ã‘ã‚‰ã‚Œã‚‹ã€‚

ã¾ã‚ã€ç§ã®ã‚ˆã†ãªå€‹äººé–‹ç™ºè€…ã«ã¯ç¸é ã„è©±ã§ã¯ã‚ã‚‹ã€‚ã§ã‚‚ã€Œå°†æ¥çš„ã«B2B SaaSã‚’ä½œã‚ŠãŸã„ã€ã¿ãŸã„ãªé‡æœ›ãŒã‚ã‚‹äººã¯ã€é ­ã®ç‰‡éš…ã«å…¥ã‚Œã¦ãŠã„ã¦æã¯ãªã„ã‹ã‚‚ã€‚

### 2. ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹è¦ä»¶ãŒã‚ã‚‹å ´åˆ

SOC 2ã€GDPRã€HIPAAãªã©ã®ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹è¦ä»¶ã‚’æŒã¤é¡§å®¢ã‚’æŠ±ãˆã‚‹å ´åˆã€å¤–éƒ¨APIã¸ã®ãƒ‡ãƒ¼ã‚¿é€ä¿¡ãŒè¨±å¯ã•ã‚Œãªã„ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ã€‚

ã“ã®è¾ºã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆç•¥èªã€æ­£ç›´ã¾ã å…¨éƒ¨ã¯ç†è§£ã§ãã¦ã„ãªã„ã€‚ã€Œãªã‚“ã‹ãƒ¤ãƒãã†ãªè¦åˆ¶ãŒã‚ã‚‹ã‚‰ã—ã„ã€ãã‚‰ã„ã®èªè­˜ã€‚è©³ã—ã„äººã€æ•™ãˆã¦ãã ã•ã„ã€‚

### 3. å¤§é‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã™ã‚‹å ´åˆ

APIã®å¾“é‡èª²é‡‘ãŒç©ã¿é‡ãªã‚‹ã¨ã€è‡ªç¤¾é‹ç”¨ã®æ–¹ãŒã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ããªã‚‹**æç›Šåˆ†å²ç‚¹**ãŒå­˜åœ¨ã™ã‚‹ã€‚ä¸€èˆ¬çš„ã«**1æ—¥ã‚ãŸã‚Š200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Š**ã‚’å‡¦ç†ã™ã‚‹å ´åˆã€ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆã®æ¤œè¨ä¾¡å€¤ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

ãŸã ã€ã“ã®ã€Œ200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨ã„ã†æ•°å­—ã¯ã‚ãã¾ã§ç›®å®‰ã€‚å®Ÿéš›ã®æç›Šåˆ†å²ç‚¹ã¯ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚„ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã«ã‚ˆã£ã¦å¤§ããå¤‰ã‚ã‚‹ã®ã§ã€ã¡ã‚ƒã‚“ã¨è‡ªåˆ†ã§è¨ˆç®—ã—ãŸæ–¹ãŒã„ã„ã€‚

ç§ã®å ´åˆã€Claude APIã®æœˆé¡ãŒã ã„ãŸã„æ•°åƒå††ç¨‹åº¦ãªã®ã§ã€ã¾ã ã¾ã APIã§ååˆ†ã€‚H100ã‚’è²·ã†é‡‘ã¯ãªã„ã€‚ã¨ã„ã†ã‹ç½®ãå ´æ‰€ã‚‚ãªã„ã€‚

### 4. ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŒå¿…è¦ãªå ´åˆ

è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ã€ç‰¹å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®è‡ªç”±ãªèª¿æ•´ãªã©ã€**APIã§ã¯åˆ¶é™ã•ã‚Œã‚‹æŸ”è»Ÿæ€§**ãŒå¿…è¦ãªå ´åˆã«æœ‰åŠ¹ã€‚

ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã€ã‚‚ã†ã¡ã‚‡ã£ã¨ã€‡ã€‡ã«è©³ã—ããªã£ã¦ã»ã—ã„ã‚“ã ã‚ˆãªãã€ã¿ãŸã„ãªæ¬²æ±‚ãŒã‚ã‚‹äººå‘ã‘ã€‚

---

## ä¸»è¦APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿å–ã‚Šæ‰±ã„ãƒãƒªã‚·ãƒ¼

ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMãŒå¿…è¦ã‹ã€ã‚’åˆ¤æ–­ã™ã‚‹å‰ã«ã€ã¾ãšã¯å„ç¤¾APIã®ãƒ‡ãƒ¼ã‚¿ãƒãƒªã‚·ãƒ¼ã‚’æ­£ç¢ºã«ç†è§£ã—ã¦ãŠã“ã†ã€‚

**ã“ã“ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å¼•ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ä¿¡é ¼æ€§ã¯é«˜ã„ã¯ãšã€‚** ã¨ã„ã†ã‹ã€ã“ã“ã§å˜˜ã‚’æ›¸ã„ãŸã‚‰è¨˜äº‹ã®æ„å‘³ãŒãªã„ã®ã§ã€ã‹ãªã‚Šæ…é‡ã«ç¢ºèªã—ãŸã€‚

### OpenAI API

| é …ç›® | å†…å®¹ |
|------|------|
| **ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¸ã®ä½¿ç”¨** | APIçµŒç”±ã®ãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„ï¼ˆã‚ªãƒ—ãƒˆã‚¤ãƒ³ã—ãªã„é™ã‚Šï¼‰ |
| **ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“** | ä¸æ­£åˆ©ç”¨ç›£è¦–ã®ãŸã‚æœ€å¤§30æ—¥é–“ä¿æŒ |
| **ã‚¼ãƒ­ãƒ‡ãƒ¼ã‚¿ä¿æŒ** | å¯©æŸ»æ‰¿èªã‚’å¾—ãŸé¡§å®¢ã®ã¿åˆ©ç”¨å¯èƒ½ |

> As of March 1, 2023, data sent to the OpenAI API is not used to train or improve OpenAI models (unless you explicitly opt in).
>
> â€” [OpenAI Data Controls](https://platform.openai.com/docs/guides/your-data)

**æ³¨æ„ç‚¹**ï¼šWebã‚„ã‚¢ãƒ—ãƒªã§ChatGPTã‚’ä½¿ã†å ´åˆï¼ˆã„ã‚ã‚†ã‚‹ã€Œãƒãƒ£ãƒƒãƒˆç”»é¢ã§ã‚„ã‚Šã¨ã‚Šã™ã‚‹ã‚„ã¤ã€ï¼‰ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚APIã¨ã¯åˆ¥ç‰©ãªã®ã§æ··åŒã—ãªã„ã‚ˆã†ã«ã€‚

ã“ã‚Œã€æ„å¤–ã¨çŸ¥ã‚‰ãªã„äººå¤šã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€‚ã€ŒChatGPTã«æ©Ÿå¯†æƒ…å ±å…¥ã‚Œã¡ã‚ƒã£ãŸ...ã€ã¨ã„ã†äº‹æ•…ã€å‰²ã¨èãã€‚

### Anthropic Claude API

| é …ç›® | å†…å®¹ |
|------|------|
| **ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¸ã®ä½¿ç”¨** | å•†ç”¨å¥‘ç´„ï¼ˆAPIã€Enterpriseç­‰ï¼‰ã§ã¯è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„ |
| **ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“** | 2025å¹´9æœˆä»¥é™ã€7æ—¥é–“ã«çŸ­ç¸®ï¼ˆå¾“æ¥ã¯30æ—¥ï¼‰ |
| **ã‚¼ãƒ­ãƒ‡ãƒ¼ã‚¿ä¿æŒ** | DPAè¿½åŠ å¥‘ç´„ã§åˆ©ç”¨å¯èƒ½ |

> Commercial Terms prohibit it entirely. API data is never used for model training.
>
> â€” [Anthropic Privacy Center](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)

**æ³¨æ„ç‚¹**ï¼šWebã‚„ã‚¢ãƒ—ãƒªã§Claudeï¼ˆclaude.aiï¼‰ã‚’ä½¿ã†å ´åˆã€2025å¹´ã®è¦ç´„æ›´æ–°ã§ã‚ªãƒ—ãƒˆã‚¤ãƒ³å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿æä¾›ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚Freeã€Proã€Maxãƒ—ãƒ©ãƒ³ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¯¾è±¡ã€‚APIåˆ©ç”¨ã«ã¯å½±éŸ¿ã—ãªã„ã€‚

ç§ã¯Claude APIã‚’æ„›ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ã“ã®ãƒãƒªã‚·ãƒ¼ã¯å®‰å¿ƒææ–™ã€‚7æ—¥é–“ä¿æŒã«çŸ­ç¸®ã•ã‚ŒãŸã®ã‚‚è‰¯ã„å‚¾å‘ã ã¨æ€ã†ã€‚

### Google Vertex AI

| é …ç›® | å†…å®¹ |
|------|------|
| **ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¸ã®ä½¿ç”¨** | é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„ |
| **ãƒ‡ãƒ¼ã‚¿ä¿æŒ** | ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æŒ‡å®šã§ãƒ‡ãƒ¼ã‚¿æ‰€åœ¨åœ°ã‚’åˆ¶å¾¡å¯èƒ½ |
| **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** | CMEKã€VPC Service Controlså¯¾å¿œ |

> As outlined in Service Specific Terms, Google won't use your data to train or fine-tune any AI/ML models without your prior permission.
>
> â€” [Vertex AI Zero Data Retention](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)

Googleã€ä¼æ¥­å‘ã‘ã¯ã—ã£ã‹ã‚Šã—ã¦ã‚‹å°è±¡ã€‚å€‹äººå‘ã‘ã®Geminiã¨ã¯åˆ¥ç‰©ã¨ã„ã†èªè­˜ã§è‰¯ã•ãã†ã€‚

### AWS Bedrock

| é …ç›® | å†…å®¹ |
|------|------|
| **ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¸ã®ä½¿ç”¨** | é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„ |
| **ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚¢ã‚¯ã‚»ã‚¹** | Anthropicã‚„Metaãªã©ã®ãƒ¢ãƒ‡ãƒ«æä¾›è€…ã¯é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ |
| **ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹** | GDPRã€HIPAAå¯¾å¿œ |

> Amazon Bedrock doesn't use your prompts and completions to train any AWS models and doesn't distribute them to third parties.
>
> â€” [AWS Bedrock Data Protection](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)

ãƒ¢ãƒ‡ãƒ«æä¾›è€…ï¼ˆAnthropicã€Metaç­‰ï¼‰ãŒAWSã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„æ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã®ã¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é¢ã§å®‰å¿ƒææ–™ã‹ãªã¨æ€ã†ã€‚ã€ŒBedrockã§claudeã‚’ä½¿ã£ãŸã‚‰ã€Anthropicã«ãƒ‡ãƒ¼ã‚¿ãŒè¡Œãã®ï¼Ÿã€ã¨ã„ã†ç–‘å•ã«å¯¾ã™ã‚‹ç­”ãˆã¯ã€ŒNoã€ã‚‰ã—ã„ã€‚

### Azure OpenAI

| é …ç›® | å†…å®¹ |
|------|------|
| **ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¸ã®ä½¿ç”¨** | é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„ |
| **ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“** | æœ€å¤§30æ—¥é–“ï¼ˆä¸æ­£åˆ©ç”¨ç›£è¦–ç›®çš„ï¼‰ |
| **ã‚¼ãƒ­ãƒ‡ãƒ¼ã‚¿ä¿æŒ** | EA/MCAå¥‘ç´„é¡§å®¢ã®ã¿åˆ©ç”¨å¯èƒ½ |
| **æ³¨æ„ç‚¹** | ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãŒä¸€æ™‚çš„ã«åœ°ç†çš„ã«ç§»å‹•ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼ˆ2025å¹´3æœˆæ›´æ–°ï¼‰ |

> Any prompts and completions you send to Azure OpenAI are not used to train or improve Microsoft's or OpenAI's foundational AI models.
>
> â€” [Azure OpenAI Data Privacy](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy)

2025å¹´3æœˆã®æ›´æ–°ã§ã€Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãŒåœ°ç†çš„ã«ç§»å‹•ã™ã‚‹å¯èƒ½æ€§ã€ãŒè¿½åŠ ã•ã‚ŒãŸã®ã¯ã€ã¡ã‚‡ã£ã¨æ°—ã«ãªã‚‹ãƒã‚¤ãƒ³ãƒˆã€‚ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¸ãƒ‡ãƒ³ã‚·ãƒ¼è¦ä»¶ãŒã‚ã‚‹ä¼æ¥­ã¯è¦ç¢ºèªã€‚

---

## ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ä¸»è¦ãƒ¢ãƒ‡ãƒ«ä¸€è¦§

ã“ã“ã‹ã‚‰ãŒæœ¬é¡Œã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã›ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’ã€å…¬å¼æƒ…å ±ã‚’å…ƒã«æ•´ç†ã—ãŸã€‚

**æ­£ç›´ã€ãƒ¢ãƒ‡ãƒ«ã®æ•°ãŒå¤šã™ãã¦å…¨éƒ¨ã¯è¿½ã„ãã‚Œãªã„ã€‚** ãªã®ã§ã€2026å¹´1æœˆæ™‚ç‚¹ã§ä¸»è¦ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã«çµã£ãŸã€‚

### å›½åˆ¥ãƒ»ç‰¹å¾´åˆ¥ã®æ—©è¦‹è¡¨

| ãƒ¢ãƒ‡ãƒ« | é–‹ç™ºå…ƒ | å›½ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | æ—¥æœ¬èª | æ¨è«–ç‰¹åŒ– | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« |
|--------|--------|-----|-----------|-----------|--------|----------|---------------|
| **Llama 4** | Meta | ğŸ‡ºğŸ‡¸ | 17Bã€œ400B+ | Llama License | â–³ | - | âœ“ |
| **Gemma 3** | Google | ğŸ‡ºğŸ‡¸ | 270Mã€œ27B | Gemma ToU | â–³ | - | âœ“ |
| **Phi-4** | Microsoft | ğŸ‡ºğŸ‡¸ | 3.8Bã€œ14B | MIT | â–³ | âœ“ | âœ“ |
| **Mistral Large 3** | Mistral AI | ğŸ‡«ğŸ‡· | 41B active / 675B total | Apache 2.0 | â–³ | - | âœ“ |
| **Qwen3** | Alibaba | ğŸ‡¨ğŸ‡³ | 0.6Bã€œ235B | Apache 2.0 | â— | âœ“ | âœ“ |
| **DeepSeek-R1** | DeepSeek | ğŸ‡¨ğŸ‡³ | 1.5Bã€œ671B | MIT | â—‹ | âœ“ | - |
| **ELYZA** | ELYZA | ğŸ‡¯ğŸ‡µ | 7Bã€œ70B | å•†ç”¨å¯ | â— | âœ“ | - |

â€» æ—¥æœ¬èªå¯¾å¿œã®è©•ä¾¡ã¯å…¬å¼æƒ…å ±ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å‚è€ƒã«ã—ãŸä¸»è¦³çš„ãªè©•ä¾¡

ã“ã†ã—ã¦è¦‹ã‚‹ã¨ã€**æ—¥æœ¬èªã«å¼·ã„ãƒ¢ãƒ‡ãƒ«ã¯ä¸­å›½è£½ãŒç›®ç«‹ã¤**ã‚“ã ã‚ˆãªã...ã€‚ã“ã®è¡¨ã‚’ä½œã‚ŠãªãŒã‚‰ã€Œã†ãƒ¼ã‚“ã€ã¨å”¸ã£ã¦ã—ã¾ã£ãŸã€‚å¾Œã§è©³ã—ãæ›¸ãã€‚

### å„ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°

#### Llama 4ï¼ˆMeta / ã‚¢ãƒ¡ãƒªã‚«ï¼‰

MetaãŒé–‹ç™ºã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ã€‚2025å¹´4æœˆã«Llama 4ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **Llama 4 Scout**: 17Bã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€16ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ§‹æˆã€‚å˜ä¸€H100 GPUã§å‹•ä½œå¯èƒ½
- **Llama 4 Maverick**: 17Bã‚¢ã‚¯ãƒ†ã‚£ãƒ– / 400Bãƒˆãƒ¼ã‚¿ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€128ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ§‹æˆ
- **Llama 4 Behemoth**: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é–‹ç™ºä¸­ã€MATH-500ã‚„GPQA Diamondã§GPT-4.5ã‚„Claude Sonnet 3.7ã‚’ä¸Šå›ã‚‹

> Llama 4 Scout and Llama 4 Maverick as the first open-weight natively multimodal models with unprecedented context length support and the first built using a mixture-of-experts (MoE) architecture.
>
> â€” [Meta AI Blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)

**ç‰¹å¾´ï¼š**
- ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒå…¥åŠ›ï¼‰
- MoEï¼ˆMixture of Expertsï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§åŠ¹ç‡çš„
- 200è¨€èªã‚µãƒãƒ¼ãƒˆï¼ˆMaverickï¼‰

**æ—¥æœ¬èªã«ã¤ã„ã¦ï¼š**
200è¨€èªã‚µãƒãƒ¼ãƒˆã¨ã¯ã„ãˆã€ã‚¢ã‚¸ã‚¢è¨€èªç‰¹åŒ–ã§ã¯ãªã„ã®ã§ã€æ—¥æœ¬èªæ€§èƒ½ã¯Qwenç³»ã«åŠ£ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã‹ãªãã¨ã„ã†å°è±¡ã€‚ã€Œ200è¨€èªå¯¾å¿œï¼ã€ã¨è¨€ã‚ã‚Œã¦ã‚‚ã€æ—¥æœ¬èªãŒãã®200è¨€èªã®ä¸­ã§ã©ã‚Œãã‚‰ã„å„ªå…ˆã•ã‚Œã¦ã„ã‚‹ã‹ã¯åˆ¥å•é¡Œã ã—ã€‚

#### Gemma 3ï¼ˆGoogle / ã‚¢ãƒ¡ãƒªã‚«ï¼‰

GoogleãŒé–‹ç™ºã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã€‚Gemini 2.0ã®æŠ€è¡“ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã‚‹ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**: 270Mã€1Bã€4Bã€12Bã€27B
- **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·**: 4Bä»¥ä¸Šã¯128Kãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå‰ä¸–ä»£ã®16å€ï¼‰
- **å¯¾å¿œè¨€èª**: 140è¨€èªä»¥ä¸Š
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«**: 4Bä»¥ä¸Šã¯ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›å¯¾å¿œ

> Gemma 3 supports over 140 languages.
>
> â€” [Google AI Gemma Docs](https://ai.google.dev/gemma/docs/core)

**ç‰¹å¾´ï¼š**
- 27Bã§ã‚‚å˜ä¸€H100ã¾ãŸã¯TPUã§å‹•ä½œ
- é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›å¯èƒ½ï¼ˆ27B: 46.4GB â†’ 21GBï¼‰
- é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½æ­è¼‰

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼š**
Gemma Terms of Useã«åŸºã¥ãã€‚å•†ç”¨åˆ©ç”¨å¯èƒ½ã ãŒã€ä½¿ç”¨åˆ¶é™æ¡é …ã‚’ä¸‹æµãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚‚é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚Apache 2.0ã»ã©è‡ªç”±ã§ã¯ãªã„ç‚¹ã«æ³¨æ„ã€‚

**ç§ã®çµŒé¨“ï¼š**
ç§ãŒCloudflare Workers AIã§è©¦ã—ãŸã®ãŒã“ã®Gemma 3ã®12Bãƒ¢ãƒ‡ãƒ«ã€‚Workers AIã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‹ã‚‰ãƒãƒãƒãƒã™ã‚‹ã ã‘ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¦ã€ã€Œã‚ã€æ€ã£ãŸã‚ˆã‚Šç°¡å˜ã˜ã‚ƒã‚“ã€ã¨æ„Ÿå‹•ã—ãŸè¨˜æ†¶ãŒã‚ã‚‹ã€‚

ãŸã ã€æ—¥æœ¬èªã§ä½¿ãŠã†ã¨ã—ãŸã‚‰å¾®å¦™ã«ä¸è‡ªç„¶ãªè¿”ç­”ãŒè¿”ã£ã¦ãã¦ã€ã€Œã‚„ã£ã±æ—¥æœ¬èªã¯å¼±ã„ã®ã‹ãª...ã€ã¨æ€ã£ãŸã€‚è‹±èªã§ä½¿ã†åˆ†ã«ã¯æ‚ªããªã‹ã£ãŸã‘ã©ã€‚

#### Phi-4ï¼ˆMicrosoft / ã‚¢ãƒ¡ãƒªã‚«ï¼‰

MicrosoftãŒé–‹ç™ºã™ã‚‹å°å‹é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€‚ã€ŒSmall Language Modelã€ã¨ã„ã†ä½ç½®ã¥ã‘ã€‚èª­ã¿æ–¹ã¯ã€Œãƒ•ã‚¡ã‚¤ã€ï¼ˆã‚®ãƒªã‚·ãƒ£æ–‡å­—ã®Ï†ï¼‰ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **Phi-4**: 14Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€è¤‡é›‘ãªæ¨è«–ï¼ˆç‰¹ã«æ•°å­¦ï¼‰ã«ç‰¹åŒ–
- **Phi-4-mini**: 3.8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€128Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€200Kèªå½™
- **Phi-4-multimodal**: 5.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ†ã‚­ã‚¹ãƒˆ+éŸ³å£°+ç”»åƒå…¥åŠ›
- **Phi-4-reasoning**: 14Bã§ã€DeepSeek-R1 distilled 70Bã‚’ä¸Šå›ã‚Šã€ãƒ•ãƒ«DeepSeek-R1ã«è¿«ã‚‹æ¨è«–æ€§èƒ½

> Phi-4-reasoning, with only 14B parameters, performs well across a wide range of reasoning tasks, outperforming significantly larger open-weight models such as DeepSeek-R1 distilled 70B model.
>
> â€” [Microsoft Tech Community](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090)

**ç‰¹å¾´ï¼š**
- **MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹**ï¼ˆå®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ï¼‰
- å°å‹ãªã®ã«æ¨è«–æ€§èƒ½ãŒé«˜ã„
- åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªå­¦ç¿’

**å‘ã„ã¦ã„ã‚‹ç”¨é€”ï¼š**
æ•°å­¦ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€è«–ç†çš„æ¨è«–ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã€‚å°å‹ãªã®ã§ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã‚„ã‚³ã‚¹ãƒˆé‡è¦–ã®ç’°å¢ƒã«é©ã—ã¦ã„ã‚‹ã‹ã‚‚ã€‚

14Bã§70Bãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã¨ã‹ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ã‚¤ãƒ³ãƒ•ãƒ¬ã«æ­¯æ­¢ã‚ã‚’ã‹ã‘ã‚‹å¸Œæœ›ã®å…‰ã‚’æ„Ÿã˜ã‚‹ã€‚ã€Œãƒ‡ã‚«ã‘ã‚Šã‚ƒæ­£ç¾©ã€ã˜ã‚ƒãªã„ã‚“ã ãªã€‚

#### Mistral Large 3ï¼ˆMistral AI / ãƒ•ãƒ©ãƒ³ã‚¹ï¼‰

ãƒ•ãƒ©ãƒ³ã‚¹ã®Mistral AIãŒé–‹ç™ºã€‚ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ç™ºã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å­˜åœ¨æ„ŸãŒã‚ã‚‹ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 41Bã‚¢ã‚¯ãƒ†ã‚£ãƒ– / 675Bãƒˆãƒ¼ã‚¿ãƒ«ï¼ˆMoEï¼‰
- **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·**: 256Kãƒˆãƒ¼ã‚¯ãƒ³
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Apache 2.0

> Mistral Large 3 is a state-of-the-art, open-weight large language model released by French AI startup Mistral AI in December 2025. It is fully open-sourced under the Apache 2.0 license.
>
> â€” [Mistral AI Docs](https://docs.mistral.ai/getting-started/models/models_overview/)

**ç‰¹å¾´ï¼š**
- ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘è¨€èªã«å¼·ã„
- å®Œå…¨Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆCodestralï¼‰ã‚‚æä¾›

**æ—¥æœ¬èªã«ã¤ã„ã¦ï¼š**
80è¨€èªä»¥ä¸Šã‚µãƒãƒ¼ãƒˆã¨ã¯ã„ãˆã€ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘è¨€èªãŒä¸»è»¸ã€‚æ—¥æœ¬èªã¯ã€Œä½¿ãˆãªãã¯ãªã„ã‘ã©ã€æœ€é©ã§ã¯ãªã„ã€ã¨ã„ã†æ„Ÿã˜ã‹ãªãã€‚ãƒ•ãƒ©ãƒ³ã‚¹èªã‚„ãƒ‰ã‚¤ãƒ„èªã§ä½¿ã„ãŸã„äººã«ã¯è‰¯ã•ãã†ã€‚

#### Qwen3ï¼ˆAlibaba / ä¸­å›½ï¼‰

AlibabaãŒé–‹ç™ºã€‚æ—¥æœ¬èªã‚’å«ã‚€ã‚¢ã‚¸ã‚¢è¨€èªã«å¼·ã„ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **Dense**: 0.6Bã€1.7Bã€4Bã€8Bã€14Bã€32B
- **MoE**: 30B-A3Bï¼ˆ3Bã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼‰ã€235B-A22Bï¼ˆ22Bã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼‰
- **å¯¾å¿œè¨€èª**: 119è¨€èª
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Apache 2.0

> Qwen3 models support 119 languages and dialects.
>
> â€” [Qwen Blog](https://qwenlm.github.io/blog/qwen3/)

**ç‰¹å¾´ï¼š**
- æ—¥æœ¬èªã€ä¸­å›½èªã€éŸ“å›½èªãªã©ã‚¢ã‚¸ã‚¢è¨€èªã«å¼·ã„
- Reasoning Modelï¼ˆæ€è€ƒã®é€£é–ï¼‰å¯¾å¿œ
- å®Œå…¨Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

**æ—¥æœ¬èªæ€§èƒ½ï¼š**
JMMLUãªã©æ—¥æœ¬èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®è©•ä¾¡ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€å…¬å¼ã«å¤šè¨€èªå¯¾å¿œã‚’è¬³ã£ã¦ã„ã‚‹ã€‚æ—¥æœ¬èªã§ä½¿ã†ãªã‚‰Qwenç³»ã¯æœ‰åŠ›ãªé¸æŠè‚¢ã€‚

**æ­£ç›´ãªæ„Ÿæƒ³ï¼š**
æ€§èƒ½ã ã‘è¦‹ãŸã‚‰ã€ŒQwenä¸€æŠã˜ã‚ƒã‚“ã€ã£ã¦æ€ã£ã¡ã‚ƒã†ã‚“ã ã‘ã©ã€ãã†å˜ç´”ã«ã„ã‹ãªã„ã®ãŒå¾Œè¿°ã™ã‚‹ã€Œå¿ƒç†çš„ãªå£ã€å•é¡Œã€‚æ‚©ã¾ã—ã„ã€‚

#### DeepSeek-R1ï¼ˆDeepSeek / ä¸­å›½ï¼‰

DeepSeekãŒé–‹ç™ºã€‚å¼·åŒ–å­¦ç¿’ã®ã¿ã§æ¨è«–èƒ½åŠ›ã‚’ç²å¾—ã—ãŸç‚¹ãŒç‰¹å¾´çš„ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **DeepSeek-R1**: OpenAI o1ã¨åŒç­‰ã®æ•°å­¦ã€ã‚³ãƒ¼ãƒ‰ã€æ¨è«–æ€§èƒ½
- **è’¸ç•™ãƒ¢ãƒ‡ãƒ«**: 1.5Bã€7Bã€8Bã€14Bã€32Bã€70Bï¼ˆQwen2.5/Llama3ãƒ™ãƒ¼ã‚¹ï¼‰
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: MIT License

> DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.
>
> â€” [DeepSeek GitHub](https://github.com/deepseek-ai/DeepSeek-R1)

**ç‰¹å¾´ï¼š**
- å¼·åŒ–å­¦ç¿’ã®ã¿ã§æ¨è«–èƒ½åŠ›ã‚’ç²å¾—ï¼ˆSFTãªã—ï¼‰
- è‡ªå·±æ¤œè¨¼ã€ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã€é•·ã„CoTç”Ÿæˆ
- MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å•†ç”¨åˆ©ç”¨å¯

**æ³¨æ„ç‚¹ï¼ˆå¾Œè¿°ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ‡¸å¿µå‚ç…§ï¼‰ï¼š**
ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã ãŒã€DeepSeekã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ã†å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãŒä¸­å›½ã«é€ä¿¡ã•ã‚Œã‚‹ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™åˆ†ã«ã¯å•é¡Œãªã„ã¯ãšã€‚

æŠ€è¡“çš„ã«ã¯ã™ã”ãé¢ç™½ã„ãƒ¢ãƒ‡ãƒ«ãªã‚“ã ã‘ã©ã€è‰²ã€…ã¨ç‰©è­°ã‚’é†¸ã—ã¦ã„ã‚‹ã®ã‚‚äº‹å®Ÿã€‚è©³ã—ãã¯å¾Œã§æ›¸ãã€‚

### SLMï¼ˆSmall Language Modelï¼‰ã¨ã¯

ã“ã“ã§ä¸€æ—¦ã€**SLMï¼ˆSmall Language Modelï¼‰**ã¨ã„ã†æ¦‚å¿µã«ã¤ã„ã¦è§¦ã‚Œã¦ãŠããŸã„ã€‚

ä¸Šã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã§Phi-4ã‚’ã€Œå°å‹é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€ã¨ç´¹ä»‹ã—ãŸã‘ã©ã€ã“ã‚Œã¯SLMã«åˆ†é¡ã•ã‚Œã‚‹ã€‚SLMã¨ã¯ã€ä¸€èˆ¬çš„ã«**1å„„ã€œ50å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ç¨‹åº¦ã®æ¯”è¼ƒçš„å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡ã™ã€‚

> Small language models (SLMs) can have anywhere from 100 million parameters to around 5 billion, whereas LLMs can have billions or even trillions of parameters.
>
> â€” [IBM: What Are Small Language Models?](https://www.ibm.com/think/topics/small-language-models)

#### SLMã®ç‰¹å¾´

| é …ç›® | SLM | LLM |
|------|-----|-----|
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°** | 1å„„ã€œ50å„„ | æ•°åå„„ã€œæ•°å…† |
| **å¿…è¦ãƒªã‚½ãƒ¼ã‚¹** | ä¸€èˆ¬çš„ãªPC/ã‚¹ãƒãƒ›ã§ã‚‚å‹•ä½œå¯èƒ½ | é«˜æ€§èƒ½GPU/ã‚¯ãƒ©ã‚¦ãƒ‰ãŒå¿…è¦ |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | ä½ã„ï¼ˆå¿œç­”ãŒé€Ÿã„ï¼‰ | æ¯”è¼ƒçš„é«˜ã„ |
| **ã‚³ã‚¹ãƒˆ** | ä½ã„ | é«˜ã„ |
| **å¾—æ„åˆ†é‡** | ç‰¹å®šã‚¿ã‚¹ã‚¯ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ | æ±ç”¨çš„ãªè¤‡é›‘ã‚¿ã‚¹ã‚¯ |

#### ä»£è¡¨çš„ãªSLMãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | é–‹ç™ºå…ƒ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç‰¹å¾´ |
|--------|--------|-----------|------|
| **Phi-4-mini** | Microsoft | 3.8B | æ¨è«–ç‰¹åŒ–ã€MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ |
| **Gemma 3** | Google | 270Mã€œ4B | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ |
| **Llama 4 Scout** | Meta | 17B active | MoEã§åŠ¹ç‡çš„ã€ã‚¹ãƒãƒ›ã§ã‚‚å‹•ä½œå¯èƒ½ |
| **Qwen3** | Alibaba | 0.6Bã€œ4B | å¤šè¨€èªå¯¾å¿œ |

> The Phi 4 mini model stands out as a state-of-the-art SLM, trained on 5 trillion tokens using innovative training methods.
>
> â€” [Hugging Face Blog](https://huggingface.co/blog/AviSoori/small-language-models)

#### ãªãœSLMãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã®ã‹

ã€Œãƒ‡ã‚«ã‘ã‚Œã°æ­£ç¾©ã€ã ã£ãŸLLMã®ä¸–ç•Œã§ã€SLMãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ç†ç”±ã¯ä¸»ã«3ã¤ï¼š

1. **ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®å®Ÿè¡Œ**: ã‚¹ãƒãƒ›ã€IoTãƒ‡ãƒã‚¤ã‚¹ã€çµ„ã¿è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ ã§å‹•ã‹ã›ã‚‹
2. **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: æ¨è«–ã‚³ã‚¹ãƒˆãŒå¤§å¹…ã«ä½ã„
3. **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”ãŒå¿…è¦ãªã‚¢ãƒ—ãƒªã«é©ã—ã¦ã„ã‚‹

ç§ã®ã‚ˆã†ã«ãƒ­ãƒ¼ã‚«ãƒ«PCã§è»½ãè©¦ã—ãŸã„äººã«ã¨ã£ã¦ã‚‚ã€SLMã¯ç¾å®Ÿçš„ãªé¸æŠè‚¢ã€‚RTX 4060ï¼ˆ8GB VRAMï¼‰ã§ã‚‚ååˆ†å‹•ã‹ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã€‚

ãŸã ã—ã€SLMã¯ã€Œä¸‡èƒ½ã€ã§ã¯ãªã„ã€‚è¤‡é›‘ãªæ¨è«–ã‚„é•·æ–‡ç”Ÿæˆã§ã¯LLMã«åŠ£ã‚‹å‚¾å‘ãŒã‚ã‚‹ã€‚**ç”¨é€”ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹**ã®ãŒæ­£è§£ã ã¨æ€ã†ã€‚

---

#### ELYZAï¼ˆELYZA / æ—¥æœ¬ï¼‰

æ±äº¬å¤§å­¦æ¾å°¾ç ”ç©¶å®¤ç™ºã®ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãŒé–‹ç™ºã€‚æ—¥æœ¬èªã«ç‰¹åŒ–ã€‚

**å…¬å¼æƒ…å ±ï¼š**
- **ELYZA-japanese-Llama-2**: 7Bã€13Bã€70Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **ELYZA-Shortcut-1.0-Qwen-32B**: Qwen2.5-32Bãƒ™ãƒ¼ã‚¹ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«
- **ELYZA-Thinking-1.0**: Reasoning Modelï¼ˆCoTå¯¾å¿œï¼‰
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: å•†ç”¨åˆ©ç”¨å¯èƒ½

> æ—¥æœ¬èªã«ã‚ˆã‚‹å¯¾è©±ãƒ»ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã«ãŠã„ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæä¾›ã™ã‚‹æµ·å¤–è£½LLMã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
>
> â€” [ELYZAå…¬å¼](https://elyza.ai/news/2024/03/12/%E3%82%B0%E3%83%AD%E3%83%BC%E3%83%90%E3%83%AB%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E5%8C%B9%E6%95%B5%E3%81%99%E3%82%8B700%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9Ellm%E3%82%92)

**ç‰¹å¾´ï¼š**
- æ—¥æœ¬èªã«å®Œå…¨ç‰¹åŒ–
- GPT-4oã«åŒ¹æ•µã™ã‚‹ã‚¹ã‚³ã‚¢ï¼ˆJMMLUã€Japanese MT-Benchç­‰ï¼‰
- æ—¥æœ¬ä¼æ¥­ãªã®ã§å¿ƒç†çš„éšœå£ãŒä½ã„

**å‘ã„ã¦ã„ã‚‹ç”¨é€”ï¼š**
æ—¥æœ¬èªã§ã®ãƒ“ã‚¸ãƒã‚¹åˆ©ç”¨ã€æ—¥æœ¬ä¼æ¥­å‘ã‘ã‚µãƒ¼ãƒ“ã‚¹ã€‚

**å€‹äººçš„ãªæ„Ÿæƒ³ï¼š**
ã€Œæ—¥æœ¬ä¼æ¥­ãŒä½œã£ãŸæ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã€ã¨ã„ã†ã ã‘ã§ã€ãªã‚“ã‹å®‰å¿ƒæ„ŸãŒã‚ã‚‹ã€‚æ€§èƒ½ãŒQwenã¨æ¯”ã¹ã¦ã©ã†ã‹ã¯æ­£ç›´ã‚ã‹ã‚‰ãªã„ã‘ã©ã€ã€Œä¸­å›½è£½ã¯é¿ã‘ãŸã„ã€ã¨ã„ã†è¦ä»¶ãŒã‚ã‚‹å ´åˆã®æ•‘ä¸–ä¸»çš„å­˜åœ¨ã‹ã‚‚ã—ã‚Œãªã„ã€‚æ¾å°¾ç ”ç™ºã¨ã„ã†ãƒ–ãƒ©ãƒ³ãƒ‰ã‚‚ä¿¡é ¼æ„Ÿã‚ã‚‹ã€‚

---

## æ—¥æœ¬èªã«å¼·ã„ãƒ­ãƒ¼ã‚«ãƒ«LLMè©³ç´°èª¿æŸ»

æ—¥æœ¬èªã§ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ã„ãŸã„å ´åˆã€ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ã¹ãã‹ã€‚ã“ã“ã§ã¯æ—¥æœ¬è£½ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã«ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚„å®Ÿç”¨æ€§ã‚’è©³ã—ãèª¿ã¹ã¦ã¿ãŸã€‚

### ELYZA è©³ç´°

#### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦

ELYZAã®ãƒ¢ãƒ‡ãƒ«ã¯**ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¶™æ‰¿**ã—ã¦ã„ã‚‹ç‚¹ã«æ³¨æ„ãŒå¿…è¦ã€‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ™ãƒ¼ã‚¹ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | å•†ç”¨åˆ©ç”¨ |
|--------|--------|-----------|----------|
| ELYZA-japanese-Llama-2-7B/13B | Llama 2 | Llama 2 Community License | âœ“ï¼ˆæ¡ä»¶ä»˜ãï¼‰ |
| Llama-3-ELYZA-JP-8B | Llama 3 | Meta Llama 3 Community License | âœ“ï¼ˆæ¡ä»¶ä»˜ãï¼‰ |
| ELYZA-Shortcut-1.0-Qwen-32B | Qwen2.5 | è¦ç¢ºèª | è¦ç¢ºèª |

> Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc.
>
> â€” [Hugging Face ELYZA](https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B)

Meta Llama Community Licenseã¯å•†ç”¨åˆ©ç”¨å¯èƒ½ã ãŒã€**æœˆé–“ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼7å„„äººä»¥ä¸Šã®å ´åˆã¯Metaã‹ã‚‰ã®ç‰¹åˆ¥è¨±å¯ãŒå¿…è¦**ã¨ã„ã†æ¡ä»¶ãŒã‚ã‚‹ã€‚ã¾ã‚ã€ã»ã¨ã‚“ã©ã®ä¼æ¥­ã«ã¯é–¢ä¿‚ãªã„è©±ã ã¨æ€ã†ã‘ã©ã€‚

#### Cloudflare Workers AIã§ä½¿ãˆã‚‹ã‹ï¼Ÿ

**çµè«–ï¼šä½¿ãˆãªã„ã€‚**

èª¿ã¹ã¦ã¿ãŸã¨ã“ã‚ã€Cloudflare Workers AIã«ELYZAãƒ¢ãƒ‡ãƒ«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã€‚æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ã€Gemma 3ï¼ˆ140è¨€èªå¯¾å¿œï¼‰ã‚„Qwen3ï¼ˆ119è¨€èªå¯¾å¿œï¼‰ãªã©ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã“ã¨ã«ãªã‚‹ã€‚

æ—¥æœ¬èªå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯**PLaMo Embedding 1B**ï¼ˆPreferred Networksè£½ï¼‰ãŒã‚ã‚‹ãŒã€ã“ã‚Œã¯ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”¨ã§ã‚ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ã¯ä½¿ãˆãªã„ã€‚

#### å¿…è¦ã‚¹ãƒšãƒƒã‚¯

Llama-3-ELYZA-JP-8Bã®å ´åˆï¼š
- **æ¨å¥¨VRAM**: 16GBä»¥ä¸Šï¼ˆFP16ã®å ´åˆï¼‰
- **é‡å­åŒ–ï¼ˆQ4ï¼‰**: 8GBç¨‹åº¦ã§å‹•ä½œå¯èƒ½
- **æ¨å¥¨GPU**: RTX 4060ä»¥ä¸Šã€ã¾ãŸã¯ M1/M2 Macï¼ˆ16GBä»¥ä¸Šï¼‰

### PLaMoï¼ˆPreferred Networks / æ—¥æœ¬ï¼‰

Preferred Networksï¼ˆPFNï¼‰ãŒé–‹ç™ºã™ã‚‹å›½ç”£LLMã€‚é‡‘èç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚„ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ãªã©ã€ç”¨é€”åˆ¥ã®ãƒ©ã‚¤ãƒ³ãƒŠãƒƒãƒ—ãŒå……å®Ÿã—ã¦ã„ã‚‹ã€‚

#### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆPLaMo Community Licenseï¼‰

> ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆã¯ãã®é–¢ä¿‚ä¼šç¤¾ã®ç›´è¿‘äº‹æ¥­å¹´åº¦ã®åå…¥åˆã¯å£²ä¸ŠãŒ10å„„å††ã‚’è¶…ãˆãªã„ã“ã¨ãŒæ¡ä»¶
>
> â€” [PLaMo Community License](https://www.preferred.jp/ja/plamo-community-license/)

**ã¤ã¾ã‚Šï¼š**
- **å¹´å•†10å„„å††æœªæº€**: ç„¡æ–™ã§å•†ç”¨åˆ©ç”¨å¯èƒ½
- **å¹´å•†10å„„å††ä»¥ä¸Š**: PFNã‹ã‚‰å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’åˆ¥é€”å–å¾—ã™ã‚‹å¿…è¦ã‚ã‚Š

ã“ã‚Œã€ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚„ä¸­å°ä¼æ¥­ã«ã¯ã‹ãªã‚Šè‰¯å¿ƒçš„ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã ã¨æ€ã†ã€‚å¤§ä¼æ¥­ã¯åˆ¥é€”å¥‘ç´„ã—ã¦ã­ã¨ã„ã†å½¢ã€‚

#### ä¸»ãªãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | ç”¨é€” | å•†ç”¨åˆ©ç”¨ |
|--------|------|----------|
| PLaMo 2 8B | æ±ç”¨ | âœ“ï¼ˆæ¡ä»¶ä»˜ãï¼‰ |
| PLaMo Prime | å•†ç”¨ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ— | æœ‰æ–™ |
| PLaMo Lite | ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ | æœ‰æ–™ |
| PLaMoç¿»è¨³ | ç¿»è¨³ç‰¹åŒ– | âœ“ï¼ˆæ¡ä»¶ä»˜ãï¼‰ |
| é‡‘èç‰¹åŒ–PLaMo | é‡‘èæ¥­ç•Œå‘ã‘ | æœ‰æ–™ |

#### Cloudflare Workers AIã§ä½¿ãˆã‚‹ã‹ï¼Ÿ

**PLaMo Embedding 1B**ã®ã¿åˆ©ç”¨å¯èƒ½ã€‚ãŸã ã—ã“ã‚Œã¯ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”¨ãªã®ã§ã€ãƒãƒ£ãƒƒãƒˆã‚„æ–‡ç« ç”Ÿæˆã«ã¯ä½¿ãˆãªã„ã€‚

### CyberAgentï¼ˆã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ / æ—¥æœ¬ï¼‰

ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé–‹ç™ºã™ã‚‹æ—¥æœ¬èªLLMã€‚OpenCALMã‚·ãƒªãƒ¼ã‚ºã‚„CyberAgentLMãªã©ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ç©æ¥µçš„ã«å…¬é–‹ã—ã¦ã„ã‚‹ã€‚

#### ä¸»ãªãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | ç‰¹å¾´ |
|--------|-----------|-----------|------|
| CyberAgentLM3-22B-Chat | 220å„„ï¼ˆ22Bï¼‰ | Apache 2.0 | å•†ç”¨åˆ©ç”¨å¯ã€æ—¥æœ¬èªç‰¹åŒ– |
| DeepSeek-R1-Distill-Qwen-14B-Japanese | 140å„„ | MIT | æ—¥æœ¬èªãƒªãƒ¼ã‚ºãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ« |
| OpenCALMã‚·ãƒªãƒ¼ã‚º | å„ç¨® | Apache 2.0 | å•†ç”¨åˆ©ç”¨å¯ |

> CyberAgentLM3-22B-Chatã¯ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã§é–‹ç™ºã•ã‚Œã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªApache License 2.0ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚
>
> â€” [ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå…¬å¼](https://www.cyberagent.co.jp/news/detail/id=30463)

**DeepSeek-R1-Distill-Qwen-14B-Japanese**ã¯ã€ä¸­å›½DeepSeekç¤¾ã®ãƒªãƒ¼ã‚ºãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ—¥æœ¬èªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§åˆ©ç”¨ã§ãã‚‹æ—¥æœ¬èªãƒªãƒ¼ã‚ºãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯å®šç•ªã«ãªã£ã¦ã„ã‚‹ã‚‰ã—ã„ã€‚

> â€” [Hugging Face: cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese](https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese)

### Stockmarkï¼ˆã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ / æ—¥æœ¬ï¼‰

ãƒ“ã‚¸ãƒã‚¹æƒ…å ±ã‚„ç‰¹è¨±æƒ…å ±ã‚’å«ã‚€å¤§è¦æ¨¡ãªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ |
|--------|-----------|-----------|
| Stockmark-13B | 130å„„ | MIT |

> â€” [Hugging Face: Stockmark-13B](https://huggingface.co/stockmark/stockmark-13b)

æ­£ç›´ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®è©•ä¾¡ã¯ã‚ã¾ã‚Šé«˜ããªã„ã¨ã„ã†è©±ã‚‚èãã€‚ã€Œçµè«–ã‚’æ€¥ãã™ãã‚‹ã€å‚¾å‘ãŒã‚ã‚‹ã‚‰ã—ã„ã€‚ç”¨é€”ã«ã‚ˆã£ã¦ã¯åˆã‚ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

### LLM-jpï¼ˆå›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ / æ—¥æœ¬ï¼‰

å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ï¼ˆNIIï¼‰ãŒä¸­å¿ƒã¨ãªã£ã¦é–‹ç™ºã—ã¦ã„ã‚‹ã€ã‚¢ã‚«ãƒ‡ãƒŸã‚¢ç™ºã®ã‚ªãƒ¼ãƒ—ãƒ³ãªæ—¥æœ¬èªLLMã€‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç‰¹å¾´ |
|--------|-----------|------|
| LLM-jp-3 172B | 1720å„„ | æœ€å¤§è¦æ¨¡ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« |
| LLM-jp-13B | 130å„„ | ç ”ç©¶é–‹ç™ºå‘ã‘ |

> â€” [Hugging Face: llm-jp](https://huggingface.co/llm-jp)

ç ”ç©¶ç›®çš„ã§ã®åˆ©ç”¨ãŒä¸»çœ¼ã ãŒã€å•†ç”¨åˆ©ç”¨ã®å¯å¦ã¯ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç¢ºèªãŒå¿…è¦ã€‚

### RakutenAIï¼ˆæ¥½å¤© / æ—¥æœ¬ï¼‰

æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—ãŒ2024å¹´3æœˆã«å…¬é–‹ã—ãŸå›½ç”£LLMã€‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ |
|--------|-----------|-----------|
| RakutenAI-7B-Instruct | 70å„„ | Apache 2.0 |

> â€” [Hugging Face: Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)

Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãªã®ã§å•†ç”¨åˆ©ç”¨ã‚‚è‡ªç”±ã€‚æ¥½å¤©ã¨ã„ã†å¤§ä¼æ¥­ãŒå‡ºã—ã¦ã„ã‚‹ã¨ã„ã†å®‰å¿ƒæ„Ÿã¯ã‚ã‚‹ã‹ã‚‚ã€‚

### æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã¾ã¨ã‚

| ãƒ¢ãƒ‡ãƒ« | é–‹ç™ºå…ƒ | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | å¹´å•†10å„„å††æœªæº€ | å¹´å•†10å„„å††ä»¥ä¸Š |
|--------|--------|-----------|---------------|---------------|
| ELYZAï¼ˆLlamaç³»ï¼‰ | ELYZA | Meta Llama License | âœ“ | âœ“ |
| PLaMo 2 8B | PFN | PLaMo Community | âœ“ | è¦å¥‘ç´„ |
| CyberAgentLM3 | CyberAgent | Apache 2.0 | âœ“ | âœ“ |
| Stockmark-13B | Stockmark | MIT | âœ“ | âœ“ |
| RakutenAI-7B | æ¥½å¤© | Apache 2.0 | âœ“ | âœ“ |

**ç§ã®æ„Ÿæƒ³ï¼š**
ã€Œä¸­å›½è£½ã¯é¿ã‘ãŸã„ã€ã§ã‚‚æ—¥æœ¬èªæ€§èƒ½ã¯æ¬²ã—ã„ã€ã¨ã„ã†å ´åˆã€ELYZAã‹CyberAgentLMãŒç¾å®Ÿçš„ãªé¸æŠè‚¢ã«ãªã‚Šãã†ã€‚ãƒ©ã‚¤ã‚»ãƒ³ã‚¹çš„ã«ã¯Apache 2.0ã®CyberAgentLMãŒä¸€ç•ªã‚·ãƒ³ãƒ—ãƒ«ã‹ãªã€‚

---

## ãƒ­ãƒ¼ã‚«ãƒ«PCã§å‹•ã‹ã™å ´åˆã®å¿…è¦ã‚¹ãƒšãƒƒã‚¯

ã€Œè‡ªåˆ†ã®PCã§LLMå‹•ã‹ã—ãŸã„ã€ã¨ã„ã†äººå‘ã‘ã«ã€å¿…è¦ã‚¹ãƒšãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ã¿ãŸã€‚

### ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºåˆ¥ã®VRAMç›®å®‰

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | FP16ï¼ˆãƒ•ãƒ«ç²¾åº¦ï¼‰ | Q4é‡å­åŒ– | æ¨å¥¨GPU |
|-------------|-----------------|---------|---------|
| 7B | ç´„17GB | ç´„4GB | RTX 4060 (8GB) ä»¥ä¸Š |
| 8B | ç´„18GB | ç´„5GB | RTX 4060 (8GB) ä»¥ä¸Š |
| 13Bã€œ14B | ç´„24GB | ç´„9GB | RTX 4070 (12GB) ä»¥ä¸Š |
| 32B | ç´„60GB | ç´„18GB | RTX 4090 (24GB) |
| 70B | ç´„140GB | ç´„40GB | è¤‡æ•°GPU or ã‚¯ãƒ©ã‚¦ãƒ‰ |

> RTX 4090ã¯ã€Œå·¥å¤«ã™ã‚Œã°30Bã‚¯ãƒ©ã‚¹ã¾ã§å‹•ã‹ã›ã‚‹ã€å€‹äººãŒè²·ãˆã‚‹æœ€å¼·ã®å®Ÿé¨“å®¤ã€ã§ã™ã€‚
>
> â€” [noteè¨˜äº‹](https://note.com/light_arnica2159/n/n7cc41581d454)

### é‡å­åŒ–ï¼ˆQuantizationï¼‰ã«ã¤ã„ã¦

ã€ŒQ4ã€ã¨ã‹ã€Œ4bité‡å­åŒ–ã€ã¨ã„ã†ã®ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è½ã¨ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã™æŠ€è¡“ã€‚ã–ã£ãã‚Šè¨€ã†ã¨ï¼š

- **FP16ï¼ˆãƒ•ãƒ«ç²¾åº¦ï¼‰**: é«˜å“è³ªã ã‘ã©ãƒ¡ãƒ¢ãƒªé£Ÿã†
- **Q8ï¼ˆ8bitï¼‰**: ã»ã¼å“è³ªå¤‰ã‚ã‚‰ãšã€ãƒ¡ãƒ¢ãƒªåŠæ¸›
- **Q4ï¼ˆ4bitï¼‰**: å“è³ªã‚„ã‚„ä½ä¸‹ã€ãƒ¡ãƒ¢ãƒª1/4ç¨‹åº¦

æ—¥å¸¸çš„ãªç”¨é€”ï¼ˆãƒãƒ£ãƒƒãƒˆã€æ–‡ç« ç”Ÿæˆãªã©ï¼‰ãªã‚‰Q4ã§ã‚‚ååˆ†å®Ÿç”¨çš„ã€‚è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã¯å“è³ªä½ä¸‹ãŒæ°—ã«ãªã‚‹ã‹ã‚‚ã€‚

### GPUåˆ¥ã®ç›®å®‰

| GPU | VRAM | å‹•ã‹ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ç›®å®‰ |
|-----|------|-------------------|
| RTX 4060 | 8GB | 7Bã€œ8Bï¼ˆQ4ï¼‰ |
| RTX 4070 | 12GB | 14Bï¼ˆQ4ï¼‰ã€7Bï¼ˆFP16ï¼‰ |
| RTX 4080 | 16GB | 14Bï¼ˆQ8ï¼‰ã€32Bï¼ˆQ4ã®ä¸€éƒ¨ï¼‰ |
| RTX 4090 | 24GB | 32Bï¼ˆQ4ï¼‰ã€14Bï¼ˆFP16ï¼‰ |
| RTX 5090 | 32GB | 32Bï¼ˆQ8ï¼‰ã€70Bï¼ˆQ4ã®ä¸€éƒ¨ï¼‰ |

### Macï¼ˆApple Siliconï¼‰ã®å ´åˆ

Macã¯ã€Œãƒ¦ãƒ‹ãƒ•ã‚¡ã‚¤ãƒ‰ãƒ¡ãƒ¢ãƒªã€ã¨ã„ã†ä»•çµ„ã¿ã§ã€CPUã¨GPUãŒãƒ¡ãƒ¢ãƒªã‚’å…±æœ‰ã—ã¦ã„ã‚‹ã€‚ã“ã‚ŒãŒLLMã«ã¯æœ‰åˆ©ã«åƒãã“ã¨ãŒã‚ã‚‹ã€‚

| Mac | ãƒ¡ãƒ¢ãƒª | å‹•ã‹ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ç›®å®‰ |
|-----|-------|-------------------|
| M1/M2 (16GB) | 16GB | 7Bã€œ8B |
| M3 Pro (36GB) | 36GB | 14Bã€œ32Bï¼ˆQ4ï¼‰ |
| M3 Max (64GB) | 64GB | 32Bã€œ70Bï¼ˆQ4ï¼‰ |
| M3 Ultra (192GB) | 192GB | 70Bä»¥ä¸Šã‚‚å¯èƒ½ |

> Macç’°å¢ƒã§ã¯ã€M1ãƒ»M2ãƒ»M3ãƒãƒƒãƒ—æ­è¼‰æ©Ÿç¨®ãŒç‰¹ã«å„ªç§€ãªæ€§èƒ½ã‚’ç™ºæ®ã—ã€çµ±åˆãƒ¡ãƒ¢ãƒªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«å®Ÿè¡ŒãŒå¯èƒ½ã§ã™ã€‚
>
> â€” [Zennè¨˜äº‹](https://zenn.dev/robustonian/articles/selection_of_gpus_for_local_llm)

ãŸã ã—ã€æ¨è«–é€Ÿåº¦ã¯NVIDIA GPUã«åŠ£ã‚‹å‚¾å‘ãŒã‚ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªå®¹é‡ã§å‹è² ã™ã‚‹ã‹ã€é€Ÿåº¦ã§å‹è² ã™ã‚‹ã‹ã¯ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚

### ç§ã®ç’°å¢ƒã§ã®ä½“æ„Ÿ

ç§ã¯RTX 4070 Ti SUPERï¼ˆ16GB VRAMï¼‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚Gemma 3ã®12Bã‚’Q4é‡å­åŒ–ã§å‹•ã‹ã™åˆ†ã«ã¯å•é¡Œãªã‹ã£ãŸã€‚ãŸã ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å¸¸ç”¨ã™ã‚‹ã‹ã¨ã„ã†ã¨...æ­£ç›´ã€Claude Codeã®æ–¹ãŒæ¥½ãªã®ã§ãã£ã¡ã‚’ä½¿ã£ã¡ã‚ƒã†ã€‚

---

## ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆæ¯”è¼ƒ

ã€Œã§ã€çµå±€ã„ãã‚‰ã‹ã‹ã‚‹ã®ï¼Ÿã€ã¨ã„ã†è©±ã€‚

### ã‚³ã‚¹ãƒˆæ¯”è¼ƒè¡¨ï¼ˆæœˆé¡ç›®å®‰ï¼‰

| ç’°å¢ƒ | åˆæœŸè²»ç”¨ | æœˆé¡ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆ | å‘ã„ã¦ã„ã‚‹äºº |
|------|---------|-------------------|-------------|
| **ãƒ­ãƒ¼ã‚«ãƒ«PCï¼ˆRTX 4090ï¼‰** | ç´„30ã€œ40ä¸‡å†† | é›»æ°—ä»£ ç´„3,000ã€œ5,000å†† | å€‹äººé–‹ç™ºè€…ã€å®Ÿé¨“ç”¨é€” |
| **ã‚¯ãƒ©ã‚¦ãƒ‰GPUï¼ˆH100ï¼‰** | ãªã— | ç´„$2,000ã€œ8,000/æœˆ | ä¸­è¦æ¨¡ãƒãƒ¼ãƒ ã€æœ¬ç•ªé‹ç”¨ |
| **Cloudflare Workers AI** | ãªã— | å¾“é‡èª²é‡‘ï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰ | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€è»½é‡ç”¨é€” |
| **APIï¼ˆClaude/GPTï¼‰** | ãªã— | ä½¿ç”¨é‡æ¬¡ç¬¬ï¼ˆæ•°åƒå††ã€œï¼‰ | ã»ã¨ã‚“ã©ã®äºº |

### æç›Šåˆ†å²ç‚¹ã®ç›®å®‰

> A private LLM starts to pay off when you process over 2 million tokens a day.
>
> â€” [LLM Total Cost of Ownership](https://www.ptolemay.com/post/llm-total-cost-of-ownership)

**1æ—¥200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Š**å‡¦ç†ã™ã‚‹ãªã‚‰ã€ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆã®æ¤œè¨ä¾¡å€¤ã‚ã‚Šã€‚ãã‚Œä»¥ä¸‹ãªã‚‰APIã§ååˆ†ã¨ã„ã†ã®ãŒä¸€èˆ¬çš„ãªè¦‹è§£ã‚‰ã—ã„ã€‚

ç§ã®å ´åˆã€Claude Codeã§1æ—¥ã«ä½¿ã†ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã›ã„ãœã„æ•°ä¸‡ã€œæ•°åä¸‡ç¨‹åº¦ã€‚APIã§å…¨ç„¶è¶³ã‚Šã‚‹ã€‚200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³/æ—¥ã£ã¦ç›¸å½“ãªé‡ã ã¨æ€ã†ã€‚

### ã‚¯ãƒ©ã‚¦ãƒ‰GPUã®ä¾¡æ ¼å‹•å‘ï¼ˆ2025å¹´ï¼‰

2025å¹´ã«å…¥ã£ã¦ã‹ã‚‰ã€ã‚¯ãƒ©ã‚¦ãƒ‰GPUã®ä¾¡æ ¼ãŒå¤§å¹…ã«ä¸‹ãŒã£ã¦ã„ã‚‹ã‚‰ã—ã„ã€‚

> AWS H100 instances dropped from approximately $7/hour to $3.90/hour in June 2025.
>
> â€” [BentoML GPU Procurement Guide](https://www.bentoml.com/blog/where-to-buy-or-rent-gpus-for-llm-inference)

AWSã®H100ãŒç´„44%å€¤ä¸‹ã’ã€‚ç«¶äº‰ãŒæ¿€åŒ–ã—ã¦ã„ã‚‹ãŠã‹ã’ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯å¬‰ã—ã„å‚¾å‘ã€‚

### éš ã‚Œã‚³ã‚¹ãƒˆã«æ³¨æ„

ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆã®å ´åˆã€GPUä»£ä»¥å¤–ã«ã‚‚ä»¥ä¸‹ã®ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ï¼š

- **é›»æ°—ä»£**: é«˜æ€§èƒ½GPUã¯æ¶ˆè²»é›»åŠ›ãŒå¤§ãã„ï¼ˆRTX 4090ã§450Wã€H100ã§700Wï¼‰
- **å†·å´**: ç™ºç†±å¯¾ç­–ã€ã‚¨ã‚¢ã‚³ãƒ³ä»£
- **é‹ç”¨äººä»¶è²»**: ãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œã€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆä½œæ¥­
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ç›£æŸ»ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹å¯¾å¿œ

> Self-hosting brings extra costs for 24/7 on-call staff, under-used GPUs, security audits, and higher electricity use. Add a 15% buffer for these overheads.
>
> â€” [Cost to Host Private LLM](https://www.aimprosoft.com/blog/cost-to-host-private-llm-2025/)

15%ã®ãƒãƒƒãƒ•ã‚¡ã‚’è¦‹è¾¼ã‚“ã§ãŠã‘ã€ã¨ã®ã“ã¨ã€‚

---

## ä¸­å›½è£½LLMã«é–¢ã™ã‚‹æ‡¸å¿µã¨ç¾å®Ÿçš„ãªè©•ä¾¡

ã•ã¦ã€ã“ã“ã‹ã‚‰ãŒæœ¬è¨˜äº‹ã§ä¸€ç•ªæ›¸ããŸã‹ã£ãŸãƒ‘ãƒ¼ãƒˆã€‚

**æ­£ç›´ã«æ›¸ãã€‚æ€§èƒ½é¢ã§ã¯ä¸­å›½è£½ãƒ¢ãƒ‡ãƒ«ï¼ˆç‰¹ã«Qwenã€DeepSeekï¼‰ãŒæ—¥æœ¬èªã§å¼·ã„ã®ã¯äº‹å®Ÿã€‚ã§ã‚‚ã€æ—¥æœ¬ä¼æ¥­ã§æ¡ç”¨ã™ã‚‹éš›ã«å¿ƒç†çš„ãªéšœå£ãŒã‚ã‚‹ã®ã‚‚äº‹å®Ÿã ã¨æ€ã†ã€‚**

ã€Œå¿ƒç†çš„ãªéšœå£ã€ã£ã¦æ›–æ˜§ãªè¨€ã„æ–¹ã ã‘ã©ã€è¦ã¯ã€Œãªã‚“ã¨ãªãä¸å®‰ã€ã€Œä¸Šã«èª¬æ˜ã—ã¥ã‚‰ã„ã€ã€Œä½•ã‹ã‚ã£ãŸæ™‚ã«è²¬ä»»å–ã‚Œãªã„ã€ã¿ãŸã„ãªæ„Ÿæƒ…ã®ã“ã¨ã€‚æŠ€è¡“çš„ãªè©±ã ã‘ã˜ã‚ƒç‰‡ä»˜ã‹ãªã„ã€çµ„ç¹”ã®æ„æ€æ±ºå®šã«ãŠã‘ã‚‹ç¾å®ŸãŒã‚ã‚‹ã€‚

### DeepSeekã®å…·ä½“çš„ãªæ‡¸å¿µäº‹é …

DeepSeekã«ã¤ã„ã¦ã¯ã€è¤‡æ•°ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç ”ç©¶è€…ã‹ã‚‰æ‡¸å¿µãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã“ã¯æ„Ÿæƒ…è«–ã§ã¯ãªãã€å®Ÿéš›ã®å ±å‘Šã«åŸºã¥ã„ãŸè©±ã€‚

| æ‡¸å¿µäº‹é … | å ±å‘Šå†…å®¹ |
|----------|----------|
| **ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ** | ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸­å›½æœ¬åœŸã®ã‚µãƒ¼ãƒãƒ¼ã«ä¿å­˜ |
| **è¨“ç·´ã¸ã®ä½¿ç”¨** | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨“ç·´åˆ©ç”¨ã‚ªãƒ—ãƒˆã‚¢ã‚¦ãƒˆä¸å¯ |
| **ã‚­ãƒ¼ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯åé›†** | å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åé›†ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ |
| **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§** | å¼±ã„æš—å·åŒ–ï¼ˆ3DESï¼‰ã€SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è„†å¼±æ€§ãŒæŒ‡æ‘˜ |
| **è¦åˆ¶å½“å±€ã®å¯¾å¿œ** | ã‚¤ã‚¿ãƒªã‚¢ã€éŸ“å›½ã§ã‚¢ãƒ—ãƒªå‰Šé™¤ãƒ»åˆ©ç”¨ç¦æ­¢æªç½® |

> DeepSeek retains data "as long as necessary" for business purposes, with analysts noting no automatic deletion schedule. All data is stored on servers in mainland China.
>
> â€” [Krebs on Security](https://krebsonsecurity.com/2025/02/experts-flag-security-privacy-risks-in-deepseek-ai-app/)

> The US Navy banned the use of DeepSeek due to "potential security and ethical concerns associated with the model's origin and usage."
>
> â€” [TechTarget](https://www.techtarget.com/searchenterpriseai/tip/Does-using-DeepSeek-create-security-risks)

ç±³æµ·è»ãŒä½¿ç”¨ç¦æ­¢ã£ã¦ã€çµæ§‹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚ã‚‹è©±ã ã‚ˆãªã...ã€‚

### ã€Œãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™ã€ã“ã¨ã®æ„å‘³

ã“ã“ã§é‡è¦ãªåŒºåˆ¥ãŒã‚ã‚‹ã€‚ã“ã‚Œã€ç§ã‚‚æœ€åˆã¯æ··ä¹±ã—ã¦ã„ãŸã€‚

**ã‚¯ãƒ©ã‚¦ãƒ‰APIçµŒç”±ã§ä½¿ã†å ´åˆï¼š**
- ãƒ‡ãƒ¼ã‚¿ã¯æä¾›å…ƒã®ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã•ã‚Œã‚‹
- ä¸­å›½ä¼æ¥­ã®APIã‚’ä½¿ã† = ãƒ‡ãƒ¼ã‚¿ãŒä¸­å›½ã«è¡Œã

**ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™å ´åˆï¼š**
- ãƒ‡ãƒ¼ã‚¿ã¯è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼å†…ã§å®Œçµ
- æ¨è«–æ™‚ã®ãƒ‡ãƒ¼ã‚¿æ¼æ´©ãƒªã‚¹ã‚¯ã¯ãªã„ï¼ˆãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‹•ã‹ã™ã ã‘ãªã®ã§ï¼‰

ã¤ã¾ã‚Šã€Qwenã‚„DeepSeekã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’**è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ã§å‹•ã‹ã™å ´åˆ**ã€æ¨è«–æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸­å›½ã«é€ä¿¡ã•ã‚Œã‚‹ã“ã¨ã¯ãªã„ã€‚

ãƒ¢ãƒ‡ãƒ«ã®ã€Œå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç”±æ¥ã€ãŒæ°—ã«ãªã‚‹äººã¯ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã‘ã©ã€ãã‚Œã¯æ¨è«–æ™‚ã®ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã¨ã¯åˆ¥ã®è©±ã€‚ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä½•ã‚’å­¦ã‚“ã§è³¢ããªã£ãŸã®ã‹ã€ã¨ã€Œç§ãŒå…¥åŠ›ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã©ã“ã«è¡Œãã®ã‹ã€ã¯ã€åˆ†ã‘ã¦è€ƒãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

### Qwenã®çŠ¶æ³

Qwenã¯æ¯”è¼ƒçš„ã‚ªãƒ¼ãƒ—ãƒ³ãªå§¿å‹¢ã‚’å–ã£ã¦ã„ã‚‹ï¼š

- ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã¯**Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**ã§å…¬é–‹
- ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦**å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œå¯èƒ½**
- è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ã§å‹•ã‹ã›ã°ã€Alibabaã«ãƒ‡ãƒ¼ã‚¿ã¯é€ä¿¡ã•ã‚Œãªã„

DeepSeekã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¨ã¯ç•°ãªã‚Šã€Qwenã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½¿ã†åˆ†ã«ã¯æŠ€è¡“çš„ãªãƒªã‚¹ã‚¯ã¯ä½ã„ã¨æ€ã†ã€‚

ãŸã ã€ã€Œä¸­å›½ä¼æ¥­ãŒä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã€ã¨ã„ã†äº‹å®Ÿã«å¯¾ã™ã‚‹å¿ƒç†çš„ãªæŠµæŠ—æ„Ÿã¯ã€çµ„ç¹”ã«ã‚ˆã£ã¦ã¯ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚ã“ã‚Œã¯æŠ€è¡“ã®å•é¡Œã˜ã‚ƒãªãã¦ã€ã‚¬ãƒãƒŠãƒ³ã‚¹ã‚„ç¤¾å†…æ”¿æ²»ã®å•é¡Œã€‚ã€ŒæŠ€è¡“çš„ã«ã¯å•é¡Œãªã„ã‚“ã§ã™ã‘ã©...ã€ã¨èª¬æ˜ã—ã¦ã‚‚ã€ã€Œã§ã‚‚ãªã...ã€ã£ã¦ãªã‚‹ä¸Šå¸ã€ã„ã‚‹ã§ã—ã‚‡ï¼Ÿ

### æ—¥æœ¬ä¼æ¥­ã¨ã—ã¦ã®ç¾å®Ÿçš„ãªé¸æŠ

æ­£ç›´ãªã¨ã“ã‚ã€ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ¤æ–­ã«ãªã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ãªã¨æ€ã†ï¼š

1. **ä¸­å›½è£½ãƒ¢ãƒ‡ãƒ«NG** â†’ ELYZAã€Llamaã€Gemmaã€Mistralã€Phi
2. **æ€§èƒ½é‡è¦–ã§ä¸­å›½è£½OKï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨ï¼‰** â†’ Qwen3ï¼ˆæ—¥æœ¬èªæœ€å¼·ã‚¯ãƒ©ã‚¹ï¼‰
3. **APIåˆ©ç”¨ã§ã‚‚ä¸­å›½è£½OK** â†’ å€‹äººåˆ©ç”¨ãªã‚‰ã‚ã‚Šã‹ã‚‚ï¼ˆä¼æ¥­åˆ©ç”¨ã¯ãŠã™ã™ã‚ã—ãªã„ï¼‰

ç§å€‹äººã¨ã—ã¦ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨å‰æã§Qwenã‚’è©¦ã—ã¦ã¿ãŸã„æ°—æŒã¡ã¯ã‚ã‚‹ã€‚ã§ã‚‚ã€ä»•äº‹ã§ä½¿ã†ã¨ãªã‚‹ã¨ã€ŒELYZAã®æ–¹ãŒèª¬æ˜ã—ã‚„ã™ã„ã‚ˆãªãã€ã¨ã‚‚æ€ã†ã€‚

---

## è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ vs ã‚¯ãƒ©ã‚¦ãƒ‰GPUç’°å¢ƒ

ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å‹•ã‹ã™å ´åˆã€ã€Œå®Œå…¨è‡ªç¤¾ç®¡ç†ã€ã¨ã€Œã‚¯ãƒ©ã‚¦ãƒ‰GPUã‚’å€Ÿã‚Šã‚‹ã€ã®2ã¤ã®é¸æŠè‚¢ãŒã‚ã‚‹ã€‚

### è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ï¼ˆã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ï¼‰

**ãƒ¡ãƒªãƒƒãƒˆï¼š**
- ãƒ‡ãƒ¼ã‚¿ãŒå®Œå…¨ã«è‡ªç¤¾å†…ã«ç•™ã¾ã‚‹
- é•·æœŸçš„ã«ã¯ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒé«˜ã„å¯èƒ½æ€§
- ç›£æŸ»ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹å¯¾å¿œãŒæ˜ç¢º

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆï¼š**
- åˆæœŸæŠ•è³‡ãŒå¤§ãã„ï¼ˆH100ã¯1å°æ•°ç™¾ä¸‡å††ï¼‰
- é‹ç”¨ãƒ»ä¿å®ˆã®å°‚é–€çŸ¥è­˜ãŒå¿…è¦
- é›»åŠ›ãƒ»å†·å´è¨­å‚™ã®ã‚³ã‚¹ãƒˆ

**å‘ã„ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ï¼š**
- é‡‘èæ©Ÿé–¢ã€åŒ»ç™‚æ©Ÿé–¢ã€æ”¿åºœæ©Ÿé–¢ãªã©å³æ ¼ãªãƒ‡ãƒ¼ã‚¿æ‰€åœ¨åœ°è¦ä»¶ãŒã‚ã‚‹
- é•·æœŸçš„ã«å¤§é‡ã®ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†ãŒè¦‹è¾¼ã¾ã‚Œã‚‹

æ­£ç›´ã€å€‹äººã‚„å°è¦æ¨¡ãƒãƒ¼ãƒ ã«ã¯ç¾å®Ÿçš„ã˜ã‚ƒãªã„é¸æŠè‚¢ã€‚ã€ŒH100è²·ã†ã‹ã€œã€ã¨ã¯ãªã‚‰ãªã„ã€‚

### ã‚¯ãƒ©ã‚¦ãƒ‰GPUç’°å¢ƒï¼ˆCloudflare Workers AIç­‰ï¼‰

#### Cloudflare Workers AIã®ãƒ‡ãƒ¼ã‚¿å–ã‚Šæ‰±ã„

> Cloudflare does not use your Customer Content to train any AI models made available on Workers AI.
>
> â€” [Cloudflare Workers AI Privacy](https://developers.cloudflare.com/workers-ai/platform/privacy/)

**å…¬å¼ãƒãƒªã‚·ãƒ¼ï¼š**
- é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œãªã„
- é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¯ä»–ã®é¡§å®¢ã¨å…±æœ‰ã•ã‚Œãªã„
- Cloudflareã¯ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªç¤¾é–‹ç™ºã—ã¦ã„ãªã„ï¼ˆç¬¬ä¸‰è€…ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ›ã‚¹ãƒˆï¼‰

ã¤ã¾ã‚Šã€ã€Œé¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«ä½¿ã‚ã‚Œã‚‹ã€å¿ƒé…ãŒãã‚‚ãã‚‚æ§‹é€ çš„ã«ãªã„ã€‚Cloudflareã¯å ´æ‰€ã‚’è²¸ã—ã¦ã„ã‚‹ã ã‘ã§ã€ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã¯ä½œã£ã¦ã„ãªã„ã‹ã‚‰ã€‚

**ç§ã®çµŒé¨“ï¼š**
Cloudflare Workers AIã§Gemma 3ï¼ˆ12Bï¼‰ã‚’è©¦ã—ãŸæ™‚ã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®ç°¡å˜ã•ã«æ„Ÿå‹•ã—ãŸã€‚Wranglerã§ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ã€æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§LLMãŒå‘¼ã¹ã‚‹ã€‚ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã£ã¦ã‚‚ã£ã¨å¤§å¤‰ã ã¨æ€ã£ã¦ãŸã€ã¨ã„ã†å…ˆå…¥è¦³ãŒå´©ã‚ŒãŸç¬é–“ã ã£ãŸã€‚

ãŸã ã€ã€Œã“ã‚Œã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨å‘¼ã‚“ã§ã„ã„ã®ã‹ï¼Ÿã€ã¨ã„ã†ç–‘å•ã¯ã‚ã‚‹ã€‚Cloudflareã®ã‚¤ãƒ³ãƒ•ãƒ©ä¸Šã§å‹•ã„ã¦ã„ã‚‹ã‚ã‘ã ã‹ã‚‰ã€ã€Œã‚»ãƒŸãƒ­ãƒ¼ã‚«ãƒ«ã€ãã‚‰ã„ã®ä½ç½®ã¥ã‘ã‹ã‚‚ã—ã‚Œãªã„ã€‚

**æ®‹ã‚‹ãƒªã‚¹ã‚¯ï¼š**
1. Cloudflareè‡ªä½“ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆã“ã‚Œã¯ä»–ã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¨åŒç­‰ï¼‰
2. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã®ã‚ã‚‹æƒ…å ±ï¼ˆæ¨è«–æ™‚ã®ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã¨ã¯åˆ¥å•é¡Œï¼‰

---

## å€‹äººæƒ…å ±ã‚’æ‰±ã†ã‚µãƒ¼ãƒ“ã‚¹ã«é©ã—ãŸé¸æŠ

æœ€å¾Œã«ã€å€‹äººæƒ…å ±ã‚’æ‰±ã†å ´åˆã®é¸æŠè‚¢ã‚’æ•´ç†ã™ã‚‹ã€‚

### æ¨å¥¨åº¦é †ã®é¸æŠè‚¢

| å„ªå…ˆåº¦ | é¸æŠè‚¢ | ç†ç”± |
|--------|--------|------|
| 1 | **è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ + ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«** | ãƒ‡ãƒ¼ã‚¿ãŒå¤–éƒ¨ã«å‡ºãªã„ |
| 2 | **AWS Bedrock / Azure OpenAI / Vertex AI** | å¤§æ‰‹ã‚¯ãƒ©ã‚¦ãƒ‰ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºç›¤ã€ãƒ¢ãƒ‡ãƒ«æä¾›è€…ã®ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ |
| 3 | **Cloudflare Workers AI** | ãƒ‡ãƒ¼ã‚¿è¨“ç·´åˆ©ç”¨ãªã—ã€ãŸã ã—ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘æ©Ÿèƒ½ã¯é™å®šçš„ã‹ã‚‚ |
| 4 | **OpenAI / Anthropic APIï¼ˆZDRä»˜ãï¼‰** | ã‚¼ãƒ­ãƒ‡ãƒ¼ã‚¿ä¿æŒã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä¿è­· |

### é¿ã‘ãŸæ–¹ãŒã„ã„é¸æŠè‚¢

- **DeepSeekã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹**ï¼šGDPRã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹å•é¡Œã€ãƒ‡ãƒ¼ã‚¿æ‰€åœ¨åœ°ãŒä¸­å›½
- **Webã‚„ã‚¢ãƒ—ãƒªã®ChatGPT/Claude**ï¼šãƒãƒ£ãƒƒãƒˆç”»é¢ã§ã‚„ã‚Šã¨ã‚Šã™ã‚‹ã‚„ã¤ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¨“ç·´ã«ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼ˆAPIåˆ©ç”¨ã¨ã¯åˆ¥ç‰©ï¼‰

å¾Œè€…ã¯æ„å¤–ã¨è¦‹è½ã¨ã—ãŒã¡ã€‚ã€ŒChatGPTã§é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¡ã‚ƒã£ãŸã€ã¿ãŸã„ãªäº‹æ•…ã€æœ¬å½“ã«æ°—ã‚’ã¤ã‘ãŸæ–¹ãŒã„ã„ã€‚

---

## ã¾ã¨ã‚

é•·ããªã£ãŸã®ã§ã€æœ€å¾Œã«ã–ã£ãã‚Šã¾ã¨ã‚ã‚‹ã€‚

| è¦³ç‚¹ | æ¨å¥¨ |
|------|------|
| **æœ€é«˜ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼** | è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼ + ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« |
| **ãƒãƒ©ãƒ³ã‚¹å‹** | AWS Bedrock / Azure OpenAI / Vertex AI |
| **æ‰‹è»½ã•** | Cloudflare Workers AI |
| **æ—¥æœ¬èªæ€§èƒ½ï¼ˆä¸­å›½è£½OKï¼‰** | Qwen3ç³» |
| **æ—¥æœ¬èªæ€§èƒ½ï¼ˆæ—¥æœ¬è£½é™å®šï¼‰** | ELYZA |
| **æ¨è«–ç‰¹åŒ–** | Phi-4ã€DeepSeek-R1ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨ï¼‰ |
| **é¿ã‘ã‚‹ã¹ã** | DeepSeekã®ã‚¯ãƒ©ã‚¦ãƒ‰APIï¼ˆå€‹äººæƒ…å ±ã‚’æ‰±ã†å ´åˆï¼‰ |

ãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã€ŒAPIã®ä»£æ›¿ã€ã§ã¯ãªãã€**ãƒ‡ãƒ¼ã‚¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãŸã‚ã®é¸æŠè‚¢**ã¨ã—ã¦æ‰ãˆã‚‹ã®ãŒè‰¯ã•ãã†ã€‚

è‡ªåˆ†ãŸã¡ã®è¦ä»¶ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã€æ€§èƒ½ã€äºˆç®—ã€å¿ƒç†çš„ãªæŠµæŠ—æ„Ÿï¼‰ã‚’æ˜ç¢ºã«ã—ãŸä¸Šã§ã€æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’é¸ã‚“ã§ã„ãã—ã‹ãªã„ã‹ãªã¨æ€ã†ã€‚

---

## ãŠã‚ã‚Šã«

è¨˜äº‹ã‚’æ›¸ãçµ‚ãˆã¦æ€ã£ãŸã®ã¯ã€ã€ŒLLMé¸å®šã€æ²¼ã ãªã€ã¨ã„ã†ã“ã¨ã€‚

æŠ€è¡“çš„ãªæ¯”è¼ƒã ã‘ã§ã‚‚å¤§å¤‰ãªã®ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒãƒªã‚·ãƒ¼ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€å›½ã®å•é¡Œã€çµ„ç¹”å†…ã®æ”¿æ²»...è€ƒãˆã‚‹ã“ã¨ãŒå¤šã™ãã‚‹ã€‚

ã§ã‚‚ã€ã“ã†ã‚„ã£ã¦æ•´ç†ã—ã¦ã¿ã‚‹ã¨ã€è‡ªåˆ†ã®ä¸­ã§åˆ¤æ–­è»¸ãŒå°‘ã—ã‚¯ãƒªã‚¢ã«ãªã£ãŸæ°—ãŒã™ã‚‹ã€‚

ã‚‚ã—é–“é•ã„ã‚„å¤ã„æƒ…å ±ãŒã‚ã‚Œã°ã€æ•™ãˆã¦ã‚‚ã‚‰ãˆã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚ã“ã®åˆ†é‡ã€å¤‰åŒ–ãŒæ—©ã™ãã¦è¿½ã„ã¤ãã®ãŒå¤§å¤‰...ã€‚

---

## å‡ºå…¸ï¼ˆå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰

### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹
- [Cloudflare Workers AI Privacy](https://developers.cloudflare.com/workers-ai/platform/privacy/)
- [OpenAI Data Controls](https://platform.openai.com/docs/guides/your-data)
- [Anthropic Privacy Center](https://privacy.claude.com/en/)
- [Google Vertex AI Zero Data Retention](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)
- [AWS Bedrock Data Protection](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)
- [Azure OpenAI Data Privacy](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy)

### ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«
- [Meta AI Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
- [Google AI Gemma](https://ai.google.dev/gemma/docs/core)
- [Microsoft Phi-4](https://huggingface.co/microsoft/phi-4)
- [Microsoft Tech Community: Phi-4](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090)
- [Mistral AI Docs](https://docs.mistral.ai/getting-started/models/models_overview/)
- [Qwen Blog](https://qwenlm.github.io/blog/qwen3/)
- [DeepSeek GitHub](https://github.com/deepseek-ai/DeepSeek-R1)

### æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
- [ELYZAå…¬å¼](https://elyza.ai/)
- [Hugging Face: ELYZA](https://huggingface.co/elyza)
- [PLaMo Community License](https://www.preferred.jp/ja/plamo-community-license/)
- [ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå…¬å¼ - CyberAgentLM3](https://www.cyberagent.co.jp/news/detail/id=30463)
- [Hugging Face: cyberagent](https://huggingface.co/cyberagent)
- [Hugging Face: Stockmark-13B](https://huggingface.co/stockmark/stockmark-13b)
- [Hugging Face: llm-jp](https://huggingface.co/llm-jp)
- [Hugging Face: Rakuten/RakutenAI-7B-instruct](https://huggingface.co/Rakuten/RakutenAI-7B-instruct)

### SLMãƒ»ã‚³ã‚¹ãƒˆé–¢é€£
- [IBM: What Are Small Language Models?](https://www.ibm.com/think/topics/small-language-models)
- [Hugging Face Blog: Small Language Models](https://huggingface.co/blog/AviSoori/small-language-models)
- [LLM Total Cost of Ownership](https://www.ptolemay.com/post/llm-total-cost-of-ownership)
- [BentoML GPU Procurement Guide](https://www.bentoml.com/blog/where-to-buy-or-rent-gpus-for-llm-inference)

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£
- [Krebs on Security: DeepSeek](https://krebsonsecurity.com/2025/02/experts-flag-security-privacy-risks-in-deepseek-ai-app/)
- [TechTarget: DeepSeek Security Risks](https://www.techtarget.com/searchenterpriseai/tip/Does-using-DeepSeek-create-security-risks)
